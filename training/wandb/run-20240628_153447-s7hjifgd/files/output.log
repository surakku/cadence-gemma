
Loading the parameters from ../model/2b-it.pt into cuda:1
Parameters loaded.
GRIFFIN
18685440
projector.proj.0.weight
projector.proj.0.bias
projector.proj.2.weight
projector.proj.2.bias
projector.proj.4.weight
projector.proj.4.bias
3
Dataset({
    features: ['image', 'question', 'answers', 'answer_type', 'answerable'],
    num_rows: 4319
})
<class 'datasets.arrow_dataset.Dataset'>
/homes/jkobza/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return self._call_impl(*args, **kwargs)
Start, validation loss: -0.03556123003363609
STEP 10 training loss: -0.03193681687116623 - eval loss: -0.03556123003363609
STEP 20 training loss: -0.145354226231575 - eval loss: -0.03556123003363609
STEP 30 training loss: -0.013976662419736385 - eval loss: -0.03556123003363609
STEP 40 training loss: -0.040430113673210144 - eval loss: -0.03556123003363609
STEP 50 training loss: -0.02727319858968258 - eval loss: -0.03556123003363609
STEP 60 training loss: -0.003722426015883684 - eval loss: -0.03556123003363609
STEP 70 training loss: -0.014362633228302002 - eval loss: -0.03556123003363609
STEP 80 training loss: -0.06572818011045456 - eval loss: -0.03556123003363609
STEP 90 training loss: -0.015029189176857471 - eval loss: -0.03556123003363609
STEP 100 training loss: -0.026275986805558205 - eval loss: -0.03556123003363609
STEP 110 training loss: -0.02000301703810692 - eval loss: -0.03556123003363609
STEP 120 training loss: -0.018042296171188354 - eval loss: -0.03556123003363609
STEP 130 training loss: -0.010805617086589336 - eval loss: -0.03556123003363609
STEP 140 training loss: -0.04122987762093544 - eval loss: -0.03556123003363609
STEP 150 training loss: -0.0224403478205204 - eval loss: -0.03556123003363609
STEP 160 training loss: -0.100041963160038 - eval loss: -0.03556123003363609
STEP 170 training loss: -0.02484639547765255 - eval loss: -0.03556123003363609
STEP 180 training loss: -0.05363157391548157 - eval loss: -0.03556123003363609
STEP 190 training loss: -0.015780646353960037 - eval loss: -0.03556123003363609
STEP 200 training loss: -0.0025575398467481136 - eval loss: -0.03556123003363609
STEP 210 training loss: -0.0378001444041729 - eval loss: -0.03556123003363609
STEP 220 training loss: -0.04526420310139656 - eval loss: -0.03556123003363609
STEP 230 training loss: -0.027335122227668762 - eval loss: -0.03556123003363609
STEP 240 training loss: -0.022581931203603745 - eval loss: -0.03556123003363609
STEP 250 training loss: -0.022590287029743195 - eval loss: -0.03556123003363609
STEP 260 training loss: -0.0300795566290617 - eval loss: -0.03556123003363609
STEP 270 training loss: -0.013390063308179379 - eval loss: -0.03556123003363609
STEP 280 training loss: -0.009713421575725079 - eval loss: -0.03556123003363609
STEP 290 training loss: -0.04145852103829384 - eval loss: -0.03556123003363609
STEP 300 training loss: -0.02503141574561596 - eval loss: -0.03556123003363609
STEP 310 training loss: -0.023885954171419144 - eval loss: -0.03556123003363609
STEP 320 training loss: -0.0194475669413805 - eval loss: -0.03556123003363609
STEP 330 training loss: -0.0522456057369709 - eval loss: -0.03556123003363609
STEP 340 training loss: -0.041403986513614655 - eval loss: -0.03556123003363609
STEP 350 training loss: -0.03233368694782257 - eval loss: -0.03556123003363609
STEP 360 training loss: -0.040616512298583984 - eval loss: -0.03556123003363609
STEP 370 training loss: -0.022435227409005165 - eval loss: -0.03556123003363609
STEP 380 training loss: -0.0006552923005074263 - eval loss: -0.03556123003363609
STEP 390 training loss: -0.02215697057545185 - eval loss: -0.03556123003363609
STEP 400 training loss: -0.03126808628439903 - eval loss: -0.03556123003363609
STEP 410 training loss: -0.0373910628259182 - eval loss: -0.03556123003363609
STEP 420 training loss: -0.04018927738070488 - eval loss: -0.03556123003363609
STEP 430 training loss: -0.06805943697690964 - eval loss: -0.03556123003363609
STEP 440 training loss: -0.012408939190208912 - eval loss: -0.03556123003363609
STEP 450 training loss: -0.04053439944982529 - eval loss: -0.03556123003363609
STEP 460 training loss: -0.020320961251854897 - eval loss: -0.03556123003363609
STEP 470 training loss: -0.056741561740636826 - eval loss: -0.03556123003363609
STEP 480 training loss: -0.10477804392576218 - eval loss: -0.03556123003363609
STEP 490 training loss: -0.041883233934640884 - eval loss: -0.03556123003363609
STEP 500 training loss: -0.03609872981905937 - eval loss: -0.03556123003363609
STEP 510 training loss: -0.03155989572405815 - eval loss: -0.03556123003363609
STEP 520 training loss: -0.020547103136777878 - eval loss: -0.03556123003363609
STEP 530 training loss: -0.09385509788990021 - eval loss: -0.03556123003363609
STEP 540 training loss: -0.02265108935534954 - eval loss: -0.03556123003363609
STEP 550 training loss: -0.017169063910841942 - eval loss: -0.03556123003363609
STEP 560 training loss: -0.03829650580883026 - eval loss: -0.03556123003363609
STEP 570 training loss: -0.005460812244564295 - eval loss: -0.03556123003363609
STEP 580 training loss: -0.007235955446958542 - eval loss: -0.03556123003363609
STEP 590 training loss: -0.02645348571240902 - eval loss: -0.03556123003363609
STEP 600 training loss: -0.00573736010119319 - eval loss: -0.03556123003363609
STEP 610 training loss: -0.025956962257623672 - eval loss: -0.03556123003363609
STEP 620 training loss: -0.0783686563372612 - eval loss: -0.03556123003363609
STEP 630 training loss: -0.04065002128481865 - eval loss: -0.03556123003363609
STEP 640 training loss: -0.03843904659152031 - eval loss: -0.03556123003363609
STEP 650 training loss: -0.01916504092514515 - eval loss: -0.03556123003363609
STEP 660 training loss: -0.04845617711544037 - eval loss: -0.03556123003363609
STEP 670 training loss: -0.022551877424120903 - eval loss: -0.03556123003363609
STEP 680 training loss: -0.05907324701547623 - eval loss: -0.03556123003363609
STEP 690 training loss: -0.025157148018479347 - eval loss: -0.03556123003363609
STEP 700 training loss: -0.0260141734033823 - eval loss: -0.03556123003363609
STEP 710 training loss: -0.03317210078239441 - eval loss: -0.03556123003363609
STEP 720 training loss: -0.059466518461704254 - eval loss: -0.03556123003363609
STEP 730 training loss: -0.015694251284003258 - eval loss: -0.03556123003363609
STEP 740 training loss: -0.05605606362223625 - eval loss: -0.03556123003363609
STEP 750 training loss: -0.027237320318818092 - eval loss: -0.03556123003363609
STEP 760 training loss: -0.05753752216696739 - eval loss: -0.03556123003363609
STEP 770 training loss: -0.03928346559405327 - eval loss: -0.03556123003363609
STEP 780 training loss: -0.023717984557151794 - eval loss: -0.03556123003363609
STEP 790 training loss: -0.006082930602133274 - eval loss: -0.03556123003363609
STEP 800 training loss: -0.023167919367551804 - eval loss: -0.03556123003363609
STEP 810 training loss: -0.014928335323929787 - eval loss: -0.03556123003363609
STEP 820 training loss: -0.028701169416308403 - eval loss: -0.03556123003363609
STEP 830 training loss: -0.029492294415831566 - eval loss: -0.03556123003363609
STEP 840 training loss: -0.011663111858069897 - eval loss: -0.03556123003363609
STEP 850 training loss: -0.013024376705288887 - eval loss: -0.03556123003363609
STEP 860 training loss: -0.04480788856744766 - eval loss: -0.03556123003363609
STEP 870 training loss: -0.024857012555003166 - eval loss: -0.03556123003363609
STEP 880 training loss: -0.03814253211021423 - eval loss: -0.03556123003363609
STEP 890 training loss: -0.044974006712436676 - eval loss: -0.03556123003363609
STEP 900 training loss: -0.02970290184020996 - eval loss: -0.03556123003363609
STEP 910 training loss: -0.005823980551213026 - eval loss: -0.03556123003363609
STEP 920 training loss: -0.037899985909461975 - eval loss: -0.03556123003363609
STEP 930 training loss: -0.04796997830271721 - eval loss: -0.03556123003363609
STEP 940 training loss: -0.03824934735894203 - eval loss: -0.03556123003363609
STEP 950 training loss: -0.018852435052394867 - eval loss: -0.03556123003363609
STEP 960 training loss: -0.021632490679621696 - eval loss: -0.03556123003363609
STEP 970 training loss: -0.03270895034074783 - eval loss: -0.03556123003363609
STEP 980 training loss: -0.026159191504120827 - eval loss: -0.03556123003363609
STEP 990 training loss: -0.07067549973726273 - eval loss: -0.03556123003363609
STEP 1000 training loss: -0.06246690824627876 - eval loss: -0.03556123003363609
STEP 1010 training loss: -0.021453596651554108 - eval loss: -0.03556123003363609
STEP 1020 training loss: -0.05455571040511131 - eval loss: -0.03556123003363609
STEP 1030 training loss: -0.04538106918334961 - eval loss: -0.03556123003363609
STEP 1040 training loss: -0.02039503864943981 - eval loss: -0.03556123003363609
STEP 1050 training loss: -0.011110729537904263 - eval loss: -0.03556123003363609
STEP 1060 training loss: -0.058587174862623215 - eval loss: -0.03556123003363609
STEP 1070 training loss: -0.030460109934210777 - eval loss: -0.03556123003363609
STEP 1080 training loss: -0.018979189917445183 - eval loss: -0.03556123003363609
STEP 1090 training loss: -0.03580830618739128 - eval loss: -0.03556123003363609
STEP 1100 training loss: -0.015544436872005463 - eval loss: -0.03556123003363609
STEP 1110 training loss: -0.007441627327352762 - eval loss: -0.03556123003363609
STEP 1120 training loss: -0.07503923773765564 - eval loss: -0.03556123003363609
STEP 1130 training loss: -0.037902940064668655 - eval loss: -0.03556123003363609
STEP 1140 training loss: -0.020810028538107872 - eval loss: -0.03556123003363609
STEP 1150 training loss: -0.04028267785906792 - eval loss: -0.03556123003363609
STEP 1160 training loss: -0.04156383499503136 - eval loss: -0.03556123003363609
STEP 1170 training loss: -0.05884846672415733 - eval loss: -0.03556123003363609
STEP 1180 training loss: -0.027184396982192993 - eval loss: -0.03556123003363609
STEP 1190 training loss: -0.01774212159216404 - eval loss: -0.03556123003363609
STEP 1200 training loss: -0.02762342058122158 - eval loss: -0.03556123003363609
STEP 1210 training loss: -0.038399867713451385 - eval loss: -0.03556123003363609
STEP 1220 training loss: -0.040360599756240845 - eval loss: -0.03556123003363609
STEP 1230 training loss: -0.028513386845588684 - eval loss: -0.03556123003363609
STEP 1240 training loss: -0.03279901668429375 - eval loss: -0.03556123003363609
STEP 1250 training loss: -0.039568740874528885 - eval loss: -0.03556123003363609
STEP 1260 training loss: -0.06954329460859299 - eval loss: -0.03556123003363609
STEP 1270 training loss: -0.041552986949682236 - eval loss: -0.03556123003363609
STEP 1280 training loss: -0.01368076540529728 - eval loss: -0.03556123003363609
STEP 1290 training loss: -0.047233741730451584 - eval loss: -0.03556123003363609
STEP 1300 training loss: -0.09105603396892548 - eval loss: -0.03556123003363609
STEP 1310 training loss: -0.01689002849161625 - eval loss: -0.03556123003363609
STEP 1320 training loss: -0.0019994245376437902 - eval loss: -0.03556123003363609
STEP 1330 training loss: -0.024611493572592735 - eval loss: -0.03556123003363609
STEP 1340 training loss: -0.047511689364910126 - eval loss: -0.03556123003363609
STEP 1350 training loss: -0.056707631796598434 - eval loss: -0.03556123003363609
STEP 1360 training loss: -0.0426083542406559 - eval loss: -0.03556123003363609
STEP 1370 training loss: -0.04007309302687645 - eval loss: -0.03556123003363609
STEP 1380 training loss: -0.03953343257308006 - eval loss: -0.03556123003363609
STEP 1390 training loss: -0.037912651896476746 - eval loss: -0.03556123003363609
STEP 1400 training loss: -0.06501896679401398 - eval loss: -0.03556123003363609
STEP 1410 training loss: -0.03114023245871067 - eval loss: -0.03556123003363609
STEP 1420 training loss: -0.03637338802218437 - eval loss: -0.03556123003363609
STEP 1430 training loss: -0.008064046502113342 - eval loss: -0.03556123003363609
STEP 1440 training loss: -0.04836774989962578 - eval loss: -0.03556123003363609
STEP 1450 training loss: -0.01719750463962555 - eval loss: -0.03556123003363609
STEP 1460 training loss: -0.05980297550559044 - eval loss: -0.03556123003363609
STEP 1470 training loss: -0.07196386903524399 - eval loss: -0.03556123003363609
STEP 1480 training loss: -0.059346701949834824 - eval loss: -0.03556123003363609
STEP 1490 training loss: -0.04412033408880234 - eval loss: -0.03556123003363609
STEP 1500 training loss: -0.010755082592368126 - eval loss: -0.03556123003363609
STEP 1510 training loss: -0.02648073434829712 - eval loss: -0.03556123003363609
STEP 1520 training loss: -0.02049066685140133 - eval loss: -0.03556123003363609
STEP 1530 training loss: -0.04112096503376961 - eval loss: -0.03556123003363609
STEP 1540 training loss: -0.044342685490846634 - eval loss: -0.03556123003363609
STEP 1550 training loss: -0.08001308143138885 - eval loss: -0.03556123003363609
STEP 1560 training loss: -0.006020091008394957 - eval loss: -0.03556123003363609
STEP 1570 training loss: -0.018479302525520325 - eval loss: -0.03556123003363609
STEP 1580 training loss: -0.04153849929571152 - eval loss: -0.03556123003363609
STEP 1590 training loss: -0.005056470166891813 - eval loss: -0.03556123003363609
STEP 1600 training loss: -0.04149460420012474 - eval loss: -0.03556123003363609
STEP 1610 training loss: -0.023610759526491165 - eval loss: -0.03556123003363609
STEP 1620 training loss: -0.028145654127001762 - eval loss: -0.03556123003363609
STEP 1630 training loss: -0.0661141648888588 - eval loss: -0.03556123003363609
STEP 1640 training loss: -0.036345429718494415 - eval loss: -0.03556123003363609
STEP 1650 training loss: -0.08710949122905731 - eval loss: -0.03556123003363609
STEP 1660 training loss: -0.06637856364250183 - eval loss: -0.03556123003363609
STEP 1670 training loss: -0.012685981579124928 - eval loss: -0.03556123003363609
STEP 1680 training loss: -0.0495731495320797 - eval loss: -0.03556123003363609
STEP 1690 training loss: -0.03534004092216492 - eval loss: -0.03556123003363609
STEP 1700 training loss: -0.03077089600265026 - eval loss: -0.03556123003363609
STEP 1710 training loss: -0.019404781982302666 - eval loss: -0.03556123003363609
STEP 1720 training loss: -0.01689232885837555 - eval loss: -0.03556123003363609
STEP 1730 training loss: -0.021811237558722496 - eval loss: -0.03556123003363609
STEP 1740 training loss: -0.0680636465549469 - eval loss: -0.03556123003363609
STEP 1750 training loss: -0.03452145308256149 - eval loss: -0.03556123003363609
STEP 1760 training loss: -0.02595539763569832 - eval loss: -0.03556123003363609
STEP 1770 training loss: -0.007946374826133251 - eval loss: -0.03556123003363609
STEP 1780 training loss: -0.08373073488473892 - eval loss: -0.03556123003363609
STEP 1790 training loss: -0.08755578100681305 - eval loss: -0.03556123003363609
STEP 1800 training loss: -0.04833315685391426 - eval loss: -0.03556123003363609
STEP 1810 training loss: -0.015058008022606373 - eval loss: -0.03556123003363609
STEP 1820 training loss: -0.06225491687655449 - eval loss: -0.03556123003363609
STEP 1830 training loss: -0.059313368052244186 - eval loss: -0.03556123003363609
STEP 1840 training loss: -0.018069343641400337 - eval loss: -0.03556123003363609
STEP 1850 training loss: -0.010737864300608635 - eval loss: -0.03556123003363609
STEP 1860 training loss: -0.019764874130487442 - eval loss: -0.03556123003363609
STEP 1870 training loss: -0.049257099628448486 - eval loss: -0.03556123003363609
STEP 1880 training loss: -0.04071006551384926 - eval loss: -0.03556123003363609
STEP 1890 training loss: -0.010308202356100082 - eval loss: -0.03556123003363609
STEP 1900 training loss: -0.039693232625722885 - eval loss: -0.03556123003363609
STEP 1910 training loss: -0.005408975761383772 - eval loss: -0.03556123003363609
STEP 1920 training loss: -0.03748907893896103 - eval loss: -0.03556123003363609
STEP 1930 training loss: -0.09526100754737854 - eval loss: -0.03556123003363609
STEP 1940 training loss: -0.033279743045568466 - eval loss: -0.03556123003363609
STEP 1950 training loss: -0.026776496320962906 - eval loss: -0.03556123003363609
STEP 1960 training loss: -0.02639126591384411 - eval loss: -0.03556123003363609
STEP 1970 training loss: -0.03301845118403435 - eval loss: -0.03556123003363609
STEP 1980 training loss: -0.09116470813751221 - eval loss: -0.03556123003363609
STEP 1990 training loss: -0.021939927712082863 - eval loss: -0.03556123003363609
STEP 2000 training loss: -0.025104312226176262 - eval loss: -0.03556123003363609
STEP 2010 training loss: -0.03498244658112526 - eval loss: -0.03556123003363609
STEP 2020 training loss: -0.03461731597781181 - eval loss: -0.03556123003363609
STEP 2030 training loss: -0.022550422698259354 - eval loss: -0.03556123003363609
STEP 2040 training loss: -0.05422605201601982 - eval loss: -0.03556123003363609
STEP 2050 training loss: -0.01876002363860607 - eval loss: -0.03556123003363609
STEP 2060 training loss: -0.006084331776946783 - eval loss: -0.03556123003363609
STEP 2070 training loss: -0.02592300809919834 - eval loss: -0.03556123003363609
STEP 2080 training loss: -0.018781809136271477 - eval loss: -0.03556123003363609
STEP 2090 training loss: -0.0290289968252182 - eval loss: -0.03556123003363609
STEP 2100 training loss: -0.033241864293813705 - eval loss: -0.03556123003363609
STEP 2110 training loss: -0.0567755289375782 - eval loss: -0.03556123003363609
STEP 2120 training loss: -0.03682580217719078 - eval loss: -0.03556123003363609
STEP 2130 training loss: -0.02187339775264263 - eval loss: -0.03556123003363609
STEP 2140 training loss: -0.032623764127492905 - eval loss: -0.03556123003363609
STEP 2150 training loss: -0.05651867389678955 - eval loss: -0.03556123003363609
STEP 2160 training loss: -0.009058489464223385 - eval loss: -0.03556123003363609
STEP 2170 training loss: -0.006924604065716267 - eval loss: -0.03556123003363609
STEP 2180 training loss: -0.053816765546798706 - eval loss: -0.03556123003363609
STEP 2190 training loss: -0.035666294395923615 - eval loss: -0.03556123003363609
STEP 2200 training loss: -0.03210870549082756 - eval loss: -0.03556123003363609
STEP 2210 training loss: -0.01037286315113306 - eval loss: -0.03556123003363609
STEP 2220 training loss: -0.025048857554793358 - eval loss: -0.03556123003363609
STEP 2230 training loss: -0.031738486140966415 - eval loss: -0.03556123003363609
STEP 2240 training loss: -0.024297034367918968 - eval loss: -0.03556123003363609
STEP 2250 training loss: -0.018860692158341408 - eval loss: -0.03556123003363609
STEP 2260 training loss: -0.07313272356987 - eval loss: -0.03556123003363609
STEP 2270 training loss: -0.008254100568592548 - eval loss: -0.03556123003363609
STEP 2280 training loss: -0.04343002662062645 - eval loss: -0.03556123003363609
STEP 2290 training loss: -0.04387804865837097 - eval loss: -0.03556123003363609
STEP 2300 training loss: -0.05202709510922432 - eval loss: -0.03556123003363609
STEP 2310 training loss: -0.03303852304816246 - eval loss: -0.03556123003363609
STEP 2320 training loss: -0.05112985521554947 - eval loss: -0.03556123003363609
STEP 2330 training loss: -0.014302256517112255 - eval loss: -0.03556123003363609
STEP 2340 training loss: -0.06883871555328369 - eval loss: -0.03556123003363609
STEP 2350 training loss: -0.015125649981200695 - eval loss: -0.03556123003363609
STEP 2360 training loss: -0.05689181759953499 - eval loss: -0.03556123003363609
STEP 2370 training loss: -0.06871052086353302 - eval loss: -0.03556123003363609
STEP 2380 training loss: -0.019768651574850082 - eval loss: -0.03556123003363609
STEP 2390 training loss: -0.03010633960366249 - eval loss: -0.03556123003363609
STEP 2400 training loss: -0.0426003597676754 - eval loss: -0.03556123003363609
STEP 2410 training loss: -0.1103827953338623 - eval loss: -0.03556123003363609
STEP 2420 training loss: -0.056640833616256714 - eval loss: -0.03556123003363609
STEP 2430 training loss: -0.02854899875819683 - eval loss: -0.03556123003363609
STEP 2440 training loss: -0.03539874032139778 - eval loss: -0.03556123003363609
STEP 2450 training loss: -0.06606274098157883 - eval loss: -0.03556123003363609
STEP 2460 training loss: -0.015358030796051025 - eval loss: -0.03556123003363609
STEP 2470 training loss: -0.026225442066788673 - eval loss: -0.03556123003363609
STEP 2480 training loss: -0.012635910883545876 - eval loss: -0.03556123003363609
STEP 2490 training loss: -0.052035342901945114 - eval loss: -0.03556123003363609
STEP 2500 training loss: -0.035463713109493256 - eval loss: -0.03556123003363609
STEP 2510 training loss: -0.0032775879371911287 - eval loss: -0.03556123003363609
STEP 2520 training loss: -0.022616200149059296 - eval loss: -0.03556123003363609
STEP 2530 training loss: -0.04329857975244522 - eval loss: -0.03556123003363609
STEP 2540 training loss: -0.025681642815470695 - eval loss: -0.03556123003363609
STEP 2550 training loss: -0.03475373610854149 - eval loss: -0.03556123003363609
STEP 2560 training loss: -0.04324910044670105 - eval loss: -0.03556123003363609
STEP 2570 training loss: -0.014010468497872353 - eval loss: -0.03556123003363609
STEP 2580 training loss: -0.05684856325387955 - eval loss: -0.03556123003363609
STEP 2590 training loss: -0.009560804814100266 - eval loss: -0.03556123003363609
STEP 2600 training loss: -0.025940794497728348 - eval loss: -0.03556123003363609
STEP 2610 training loss: -0.02085600234568119 - eval loss: -0.03556123003363609
STEP 2620 training loss: -0.02271893061697483 - eval loss: -0.03556123003363609
STEP 2630 training loss: -0.011489501222968102 - eval loss: -0.03556123003363609
STEP 2640 training loss: -0.006222150754183531 - eval loss: -0.03556123003363609
STEP 2650 training loss: -0.03921864926815033 - eval loss: -0.03556123003363609
STEP 2660 training loss: -0.03055231086909771 - eval loss: -0.03556123003363609
STEP 2670 training loss: -0.02478998713195324 - eval loss: -0.03556123003363609
STEP 2680 training loss: -0.09908236563205719 - eval loss: -0.03556123003363609
STEP 2690 training loss: -0.05458131432533264 - eval loss: -0.03556123003363609
STEP 2700 training loss: -0.026877790689468384 - eval loss: -0.03556123003363609
STEP 2710 training loss: -0.019095221534371376 - eval loss: -0.03556123003363609
STEP 2720 training loss: -0.025488242506980896 - eval loss: -0.03556123003363609
STEP 2730 training loss: -0.06635042279958725 - eval loss: -0.03556123003363609
STEP 2740 training loss: -0.018066246062517166 - eval loss: -0.03556123003363609
STEP 2750 training loss: -0.0533340685069561 - eval loss: -0.03556123003363609
STEP 2760 training loss: -0.02437291108071804 - eval loss: -0.03556123003363609
STEP 2770 training loss: -0.0042653209529817104 - eval loss: -0.03556123003363609
STEP 2780 training loss: -0.010275247506797314 - eval loss: -0.03556123003363609
STEP 2790 training loss: -0.007084760349243879 - eval loss: -0.03556123003363609
STEP 2800 training loss: -0.059815913438797 - eval loss: -0.03556123003363609
STEP 2810 training loss: -0.03548867627978325 - eval loss: -0.03556123003363609
STEP 2820 training loss: -0.07447829097509384 - eval loss: -0.03556123003363609
STEP 2830 training loss: -0.01394533272832632 - eval loss: -0.03556123003363609
STEP 2840 training loss: -0.017779570072889328 - eval loss: -0.03556123003363609
STEP 2850 training loss: -0.10364575684070587 - eval loss: -0.03556123003363609
STEP 2860 training loss: -0.024035092443227768 - eval loss: -0.03556123003363609
STEP 2870 training loss: -0.040492232888936996 - eval loss: -0.03556123003363609
STEP 2880 training loss: -0.023032858967781067 - eval loss: -0.03556123003363609
STEP 2890 training loss: -0.03989196568727493 - eval loss: -0.03556123003363609
STEP 2900 training loss: -0.004692903719842434 - eval loss: -0.03556123003363609
STEP 2910 training loss: -0.01203811913728714 - eval loss: -0.03556123003363609
STEP 2920 training loss: -0.043889306485652924 - eval loss: -0.03556123003363609
STEP 2930 training loss: -0.0123513238504529 - eval loss: -0.03556123003363609
STEP 2940 training loss: -0.01190760638564825 - eval loss: -0.03556123003363609
STEP 2950 training loss: -0.02118205837905407 - eval loss: -0.03556123003363609
STEP 2960 training loss: -0.02449156530201435 - eval loss: -0.03556123003363609
STEP 2970 training loss: -0.0324067696928978 - eval loss: -0.03556123003363609
STEP 2980 training loss: -0.04969039559364319 - eval loss: -0.03556123003363609
STEP 2990 training loss: -0.06112053617835045 - eval loss: -0.03556123003363609
STEP 3000 training loss: -0.01508870255202055 - eval loss: -0.03556123003363609
STEP 3010 training loss: -0.014368860982358456 - eval loss: -0.03556123003363609
STEP 3020 training loss: -0.021258460357785225 - eval loss: -0.03556123003363609
STEP 3030 training loss: -0.027433007955551147 - eval loss: -0.03556123003363609
STEP 3040 training loss: -0.01304964441806078 - eval loss: -0.03556123003363609
STEP 3050 training loss: -0.026924235746264458 - eval loss: -0.03556123003363609
STEP 3060 training loss: -0.041690003126859665 - eval loss: -0.03556123003363609
STEP 3070 training loss: -0.05254284664988518 - eval loss: -0.03556123003363609
STEP 3080 training loss: -0.006792926695197821 - eval loss: -0.03556123003363609
STEP 3090 training loss: -0.009221683256328106 - eval loss: -0.03556123003363609
STEP 3100 training loss: -0.04038144275546074 - eval loss: -0.03556123003363609
STEP 3110 training loss: -0.03464275225996971 - eval loss: -0.03556123003363609
STEP 3120 training loss: -0.0047350795939564705 - eval loss: -0.03556123003363609
STEP 3130 training loss: -0.045306093990802765 - eval loss: -0.03556123003363609
STEP 3140 training loss: -0.024277346208691597 - eval loss: -0.03556123003363609
STEP 3150 training loss: -0.03986267000436783 - eval loss: -0.03556123003363609
STEP 3160 training loss: -0.048355165868997574 - eval loss: -0.03556123003363609
STEP 3170 training loss: -0.05338788777589798 - eval loss: -0.03556123003363609
STEP 3180 training loss: -0.038433849811553955 - eval loss: -0.03556123003363609
STEP 3190 training loss: -0.10247733443975449 - eval loss: -0.03556123003363609
STEP 3200 training loss: -0.017071250826120377 - eval loss: -0.03556123003363609
STEP 3210 training loss: -0.04366633668541908 - eval loss: -0.03556123003363609
STEP 3220 training loss: -0.06347009539604187 - eval loss: -0.03556123003363609
STEP 3230 training loss: -0.02520897425711155 - eval loss: -0.03556123003363609
STEP 3240 training loss: -0.022759264335036278 - eval loss: -0.03556123003363609
STEP 3250 training loss: -0.043605249375104904 - eval loss: -0.03556123003363609
STEP 3260 training loss: -0.0026628391351550817 - eval loss: -0.03556123003363609
STEP 3270 training loss: -0.05305841192603111 - eval loss: -0.03556123003363609
STEP 3280 training loss: -0.0212277602404356 - eval loss: -0.03556123003363609
STEP 3290 training loss: -0.04428136348724365 - eval loss: -0.03556123003363609
STEP 3300 training loss: -0.024573974311351776 - eval loss: -0.03556123003363609
STEP 3310 training loss: -0.0360511839389801 - eval loss: -0.03556123003363609
STEP 3320 training loss: -0.07288675755262375 - eval loss: -0.03556123003363609
STEP 3330 training loss: -0.032796189188957214 - eval loss: -0.03556123003363609
STEP 3340 training loss: -0.047350406646728516 - eval loss: -0.03556123003363609
STEP 3350 training loss: -0.020015783607959747 - eval loss: -0.03556123003363609
STEP 3360 training loss: -0.018880022689700127 - eval loss: -0.03556123003363609
STEP 3370 training loss: -0.06583067029714584 - eval loss: -0.03556123003363609
STEP 3380 training loss: -0.015350322239100933 - eval loss: -0.03556123003363609
STEP 3390 training loss: -0.024932993575930595 - eval loss: -0.03556123003363609
STEP 3400 training loss: -0.050170399248600006 - eval loss: -0.03556123003363609
STEP 3410 training loss: -0.004965909756720066 - eval loss: -0.03556123003363609
STEP 3420 training loss: -0.09596259146928787 - eval loss: -0.03556123003363609
STEP 3430 training loss: -0.07639771699905396 - eval loss: -0.03556123003363609
STEP 3440 training loss: -0.016309086233377457 - eval loss: -0.03556123003363609
STEP 3450 training loss: -0.028316358104348183 - eval loss: -0.03556123003363609
STEP 3460 training loss: -0.025280727073550224 - eval loss: -0.03556123003363609
STEP 3470 training loss: -0.038267843425273895 - eval loss: -0.03556123003363609
STEP 3480 training loss: -0.025116367265582085 - eval loss: -0.03556123003363609
STEP 3490 training loss: -0.06876863539218903 - eval loss: -0.03556123003363609
STEP 3500 training loss: -0.03217414394021034 - eval loss: -0.03556123003363609
STEP 3510 training loss: -0.04967079684138298 - eval loss: -0.03556123003363609
STEP 3520 training loss: -0.02955400012433529 - eval loss: -0.03556123003363609
STEP 3530 training loss: -0.02339213900268078 - eval loss: -0.03556123003363609
STEP 3540 training loss: -0.09779081493616104 - eval loss: -0.03556123003363609
STEP 3550 training loss: -0.024252664297819138 - eval loss: -0.03556123003363609
STEP 3560 training loss: -0.025852946564555168 - eval loss: -0.03556123003363609
STEP 3570 training loss: -0.022477850317955017 - eval loss: -0.03556123003363609
STEP 3580 training loss: -0.004403855185955763 - eval loss: -0.03556123003363609
STEP 3590 training loss: -0.023834316059947014 - eval loss: -0.03556123003363609
STEP 3600 training loss: -0.028654856607317924 - eval loss: -0.03556123003363609
STEP 3610 training loss: -0.027660498395562172 - eval loss: -0.03556123003363609
STEP 3620 training loss: -0.0312914215028286 - eval loss: -0.03556123003363609
STEP 3630 training loss: -0.026159003376960754 - eval loss: -0.03556123003363609
STEP 3640 training loss: -0.06967034935951233 - eval loss: -0.03556123003363609
STEP 3650 training loss: -0.040874384343624115 - eval loss: -0.03556123003363609
STEP 3660 training loss: -0.005628025159239769 - eval loss: -0.03556123003363609
STEP 3670 training loss: -0.06578593701124191 - eval loss: -0.03556123003363609
STEP 3680 training loss: -0.01048910990357399 - eval loss: -0.03556123003363609
STEP 3690 training loss: -0.00965476781129837 - eval loss: -0.03556123003363609
STEP 3700 training loss: -0.011017768643796444 - eval loss: -0.03556123003363609
STEP 3710 training loss: -0.02142207883298397 - eval loss: -0.03556123003363609
STEP 3720 training loss: -0.028265897184610367 - eval loss: -0.03556123003363609
STEP 3730 training loss: -0.018132200464606285 - eval loss: -0.03556123003363609
STEP 3740 training loss: -0.02547556161880493 - eval loss: -0.03556123003363609
STEP 3750 training loss: -0.023365167900919914 - eval loss: -0.03556123003363609
STEP 3760 training loss: -0.045124948024749756 - eval loss: -0.03556123003363609
STEP 3770 training loss: -0.07603317499160767 - eval loss: -0.03556123003363609
STEP 3780 training loss: -0.02297358028590679 - eval loss: -0.03556123003363609
STEP 3790 training loss: -0.044195130467414856 - eval loss: -0.03556123003363609
STEP 3800 training loss: -0.044822584837675095 - eval loss: -0.03556123003363609
STEP 3810 training loss: -0.02329329587519169 - eval loss: -0.03556123003363609
STEP 3820 training loss: -0.06531665474176407 - eval loss: -0.03556123003363609
STEP 3830 training loss: -0.027201402932405472 - eval loss: -0.03556123003363609
STEP 3840 training loss: -0.06995058804750443 - eval loss: -0.03556123003363609
STEP 3850 training loss: -0.022886529564857483 - eval loss: -0.03556123003363609
STEP 3860 training loss: -0.010634757578372955 - eval loss: -0.03556123003363609
STEP 3870 training loss: -0.038338370621204376 - eval loss: -0.03556123003363609
STEP 3880 training loss: -0.0368308387696743 - eval loss: -0.03556123003363609
STEP 3890 training loss: -0.019135575741529465 - eval loss: -0.03556123003363609
STEP 3900 training loss: -0.03636782988905907 - eval loss: -0.03556123003363609
STEP 3910 training loss: -0.019466426223516464 - eval loss: -0.03556123003363609
STEP 3920 training loss: -0.06331072002649307 - eval loss: -0.03556123003363609
STEP 3930 training loss: -0.04423455893993378 - eval loss: -0.03556123003363609
STEP 3940 training loss: -0.04904470965266228 - eval loss: -0.03556123003363609
STEP 3950 training loss: -0.07596495002508163 - eval loss: -0.03556123003363609
STEP 3960 training loss: -0.041997071355581284 - eval loss: -0.03556123003363609
STEP 3970 training loss: -0.022551892325282097 - eval loss: -0.03556123003363609
STEP 3980 training loss: -0.02679465152323246 - eval loss: -0.03556123003363609
STEP 3990 training loss: -0.028572946786880493 - eval loss: -0.03556123003363609
STEP 4000 training loss: -0.03596953675150871 - eval loss: -0.03556123003363609
STEP 4010 training loss: -0.05910859629511833 - eval loss: -0.03556123003363609
STEP 4020 training loss: -0.034660931676626205 - eval loss: -0.03556123003363609
STEP 4030 training loss: -0.1211119070649147 - eval loss: -0.03556123003363609
STEP 4040 training loss: -0.016302479431033134 - eval loss: -0.03556123003363609
STEP 4050 training loss: -0.013818844221532345 - eval loss: -0.03556123003363609
STEP 4060 training loss: -0.04119648411870003 - eval loss: -0.03556123003363609
STEP 4070 training loss: -0.10212311893701553 - eval loss: -0.03556123003363609
STEP 4080 training loss: -0.028592810034751892 - eval loss: -0.03556123003363609
STEP 4090 training loss: -0.033081650733947754 - eval loss: -0.03556123003363609
STEP 4100 training loss: -0.057180341333150864 - eval loss: -0.03556123003363609
STEP 4110 training loss: -0.05879642441868782 - eval loss: -0.03556123003363609
STEP 4120 training loss: -0.03264671564102173 - eval loss: -0.03556123003363609
STEP 4130 training loss: -0.029137546196579933 - eval loss: -0.03556123003363609
STEP 4140 training loss: -0.024189960211515427 - eval loss: -0.03556123003363609
STEP 4150 training loss: -0.017328521236777306 - eval loss: -0.03556123003363609
STEP 4160 training loss: -0.024516277015209198 - eval loss: -0.03556123003363609
STEP 4170 training loss: -0.06495928019285202 - eval loss: -0.03556123003363609
STEP 4180 training loss: -0.03788197040557861 - eval loss: -0.03556123003363609
STEP 4190 training loss: -0.04690885916352272 - eval loss: -0.03556123003363609
STEP 4200 training loss: -0.013123969547450542 - eval loss: -0.03556123003363609
STEP 4210 training loss: -0.03361058980226517 - eval loss: -0.03556123003363609
STEP 4220 training loss: -0.005103380884975195 - eval loss: -0.03556123003363609
STEP 4230 training loss: -0.023537492379546165 - eval loss: -0.03556123003363609
STEP 4240 training loss: -0.03148462250828743 - eval loss: -0.03556123003363609
STEP 4250 training loss: -0.031576745212078094 - eval loss: -0.03556123003363609
STEP 4260 training loss: -0.011530703864991665 - eval loss: -0.03556123003363609
STEP 4270 training loss: -0.08530619740486145 - eval loss: -0.03556123003363609
STEP 4280 training loss: -0.10678905248641968 - eval loss: -0.03556123003363609
STEP 4290 training loss: -0.004854855593293905 - eval loss: -0.03556123003363609
STEP 4300 training loss: -0.030533775687217712 - eval loss: -0.03556123003363609
STEP 4310 training loss: -0.030192656442523003 - eval loss: -0.03556123003363609
STEP 4320 training loss: -0.014653111808001995 - eval loss: -0.03556123003363609
STEP 4330 training loss: -0.03841399401426315 - eval loss: -0.03556123003363609
STEP 4340 training loss: -0.04162244126200676 - eval loss: -0.03556123003363609
STEP 4350 training loss: -0.017530230805277824 - eval loss: -0.03556123003363609
STEP 4360 training loss: -0.028959795832633972 - eval loss: -0.03556123003363609
STEP 4370 training loss: -0.04384170100092888 - eval loss: -0.03556123003363609
STEP 4380 training loss: -0.009101753123104572 - eval loss: -0.03556123003363609
STEP 4390 training loss: -0.030670691281557083 - eval loss: -0.03556123003363609
STEP 4400 training loss: -0.015523451380431652 - eval loss: -0.03556123003363609
STEP 4410 training loss: -0.046343423426151276 - eval loss: -0.03556123003363609
STEP 4420 training loss: -0.04589375481009483 - eval loss: -0.03556123003363609
STEP 4430 training loss: -0.0018110558157786727 - eval loss: -0.03556123003363609
STEP 4440 training loss: -0.0460050106048584 - eval loss: -0.03556123003363609
STEP 4450 training loss: -0.024156901985406876 - eval loss: -0.03556123003363609
STEP 4460 training loss: -0.051193322986364365 - eval loss: -0.03556123003363609
STEP 4470 training loss: -0.04572410136461258 - eval loss: -0.03556123003363609
STEP 4480 training loss: -0.02725173346698284 - eval loss: -0.03556123003363609
STEP 4490 training loss: -0.054046668112277985 - eval loss: -0.03556123003363609
STEP 4500 training loss: -0.017244918271899223 - eval loss: -0.03556123003363609
STEP 4510 training loss: -0.009036922827363014 - eval loss: -0.03556123003363609
STEP 4520 training loss: -0.03338657692074776 - eval loss: -0.03556123003363609
STEP 4530 training loss: -0.041115034371614456 - eval loss: -0.03556123003363609
STEP 4540 training loss: -0.05002765730023384 - eval loss: -0.03556123003363609
STEP 4550 training loss: -0.013494747690856457 - eval loss: -0.03556123003363609
STEP 4560 training loss: -0.07300332188606262 - eval loss: -0.03556123003363609
STEP 4570 training loss: -0.01931421458721161 - eval loss: -0.03556123003363609
STEP 4580 training loss: -0.00342562492005527 - eval loss: -0.03556123003363609
STEP 4590 training loss: -0.011836342513561249 - eval loss: -0.03556123003363609
STEP 4600 training loss: -0.038535308092832565 - eval loss: -0.03556123003363609
STEP 4610 training loss: -0.03364244103431702 - eval loss: -0.03556123003363609
STEP 4620 training loss: -0.012704441323876381 - eval loss: -0.03556123003363609
STEP 4630 training loss: -0.04046102240681648 - eval loss: -0.03556123003363609
STEP 4640 training loss: -0.04519446939229965 - eval loss: -0.03556123003363609
STEP 4650 training loss: -0.029413864016532898 - eval loss: -0.03556123003363609
STEP 4660 training loss: -0.06037694960832596 - eval loss: -0.03556123003363609
STEP 4670 training loss: -0.023249514400959015 - eval loss: -0.03556123003363609
STEP 4680 training loss: -0.01598779298365116 - eval loss: -0.03556123003363609
STEP 4690 training loss: -0.015040050260722637 - eval loss: -0.03556123003363609
STEP 4700 training loss: -0.03605632483959198 - eval loss: -0.03556123003363609
STEP 4710 training loss: -0.03420575335621834 - eval loss: -0.03556123003363609
STEP 4720 training loss: -0.07699452340602875 - eval loss: -0.03556123003363609
STEP 4730 training loss: -0.0519789457321167 - eval loss: -0.03556123003363609
STEP 4740 training loss: -0.020194513723254204 - eval loss: -0.03556123003363609
STEP 4750 training loss: -0.02284124866127968 - eval loss: -0.03556123003363609
STEP 4760 training loss: -0.009359489195048809 - eval loss: -0.03556123003363609
STEP 4770 training loss: -0.022476796060800552 - eval loss: -0.03556123003363609
STEP 4780 training loss: -0.016345908865332603 - eval loss: -0.03556123003363609
STEP 4790 training loss: -0.05342615395784378 - eval loss: -0.03556123003363609
STEP 4800 training loss: -0.013722031377255917 - eval loss: -0.03556123003363609
STEP 4810 training loss: -0.019975116476416588 - eval loss: -0.03556123003363609
STEP 4820 training loss: -0.0023535427171736956 - eval loss: -0.03556123003363609
STEP 4830 training loss: -0.04178333282470703 - eval loss: -0.03556123003363609
STEP 4840 training loss: -0.047577567398548126 - eval loss: -0.03556123003363609
STEP 4850 training loss: -0.008424862287938595 - eval loss: -0.03556123003363609
STEP 4860 training loss: -0.015630753710865974 - eval loss: -0.03556123003363609
STEP 4870 training loss: -0.04026203230023384 - eval loss: -0.03556123003363609
STEP 4880 training loss: -0.051814962178468704 - eval loss: -0.03556123003363609
STEP 4890 training loss: -0.012228925712406635 - eval loss: -0.03556123003363609
STEP 4900 training loss: -0.012602701783180237 - eval loss: -0.03556123003363609
STEP 4910 training loss: -0.021736297756433487 - eval loss: -0.03556123003363609
STEP 4920 training loss: -0.018696455284953117 - eval loss: -0.03556123003363609
STEP 4930 training loss: -0.04564565792679787 - eval loss: -0.03556123003363609
STEP 4940 training loss: -0.015038860030472279 - eval loss: -0.03556123003363609
STEP 4950 training loss: -0.040925074368715286 - eval loss: -0.03556123003363609
STEP 4960 training loss: -0.023508217185735703 - eval loss: -0.03556123003363609
STEP 4970 training loss: -0.05362702161073685 - eval loss: -0.03556123003363609
STEP 4980 training loss: -0.020308367908000946 - eval loss: -0.03556123003363609
STEP 4990 training loss: -0.03456735610961914 - eval loss: -0.03556123003363609
STEP 5000 training loss: -0.057511068880558014 - eval loss: -0.03556123003363609
STEP 5010 training loss: -0.028278792276978493 - eval loss: -0.03556123003363609
STEP 5020 training loss: -0.024235038086771965 - eval loss: -0.03556123003363609
STEP 5030 training loss: -0.045312587171792984 - eval loss: -0.03556123003363609
STEP 5040 training loss: -0.037071727216243744 - eval loss: -0.03556123003363609
STEP 5050 training loss: -0.011088103987276554 - eval loss: -0.03556123003363609
STEP 5060 training loss: -0.022157516330480576 - eval loss: -0.03556123003363609
STEP 5070 training loss: -0.02846294455230236 - eval loss: -0.03556123003363609
STEP 5080 training loss: -0.01619737409055233 - eval loss: -0.03556123003363609
STEP 5090 training loss: -0.016215531155467033 - eval loss: -0.03556123003363609
STEP 5100 training loss: -0.05960632488131523 - eval loss: -0.03556123003363609
STEP 5110 training loss: -0.0038349491078406572 - eval loss: -0.03556123003363609
STEP 5120 training loss: -0.03337971493601799 - eval loss: -0.03556123003363609
STEP 5130 training loss: -0.04533599317073822 - eval loss: -0.03556123003363609
STEP 5140 training loss: -0.02860528603196144 - eval loss: -0.03556123003363609
STEP 5150 training loss: -0.019062509760260582 - eval loss: -0.03556123003363609
STEP 5160 training loss: -0.03568192943930626 - eval loss: -0.03556123003363609
STEP 5170 training loss: -0.019642237573862076 - eval loss: -0.03556123003363609
STEP 5180 training loss: -0.024114344269037247 - eval loss: -0.03556123003363609
STEP 5190 training loss: -0.00419438723474741 - eval loss: -0.03556123003363609
STEP 5200 training loss: -0.05356508493423462 - eval loss: -0.03556123003363609
STEP 5210 training loss: -0.05051686242222786 - eval loss: -0.03556123003363609
STEP 5220 training loss: -0.013917109929025173 - eval loss: -0.03556123003363609
STEP 5230 training loss: -0.013133257627487183 - eval loss: -0.03556123003363609
STEP 5240 training loss: -0.035334713757038116 - eval loss: -0.03556123003363609
STEP 5250 training loss: -0.020761758089065552 - eval loss: -0.03556123003363609
STEP 5260 training loss: -0.019933784380555153 - eval loss: -0.03556123003363609
STEP 5270 training loss: -0.04023798927664757 - eval loss: -0.03556123003363609
STEP 5280 training loss: -0.010668555274605751 - eval loss: -0.03556123003363609
STEP 5290 training loss: -0.046177349984645844 - eval loss: -0.03556123003363609
STEP 5300 training loss: -0.025605356320738792 - eval loss: -0.03556123003363609
STEP 5310 training loss: -0.015498471446335316 - eval loss: -0.03556123003363609
STEP 5320 training loss: -0.045919373631477356 - eval loss: -0.03556123003363609
STEP 5330 training loss: -0.07734997570514679 - eval loss: -0.03556123003363609
STEP 5340 training loss: -0.05773205682635307 - eval loss: -0.03556123003363609
STEP 5350 training loss: -0.0921185165643692 - eval loss: -0.03556123003363609
STEP 5360 training loss: -0.020243750885128975 - eval loss: -0.03556123003363609
STEP 5370 training loss: -0.027170509099960327 - eval loss: -0.03556123003363609
STEP 5380 training loss: -0.02055216021835804 - eval loss: -0.03556123003363609
STEP 5390 training loss: -0.041513994336128235 - eval loss: -0.03556123003363609
STEP 5400 training loss: -0.018046785145998 - eval loss: -0.03556123003363609
STEP 5410 training loss: -0.0047991895116865635 - eval loss: -0.03556123003363609
STEP 5420 training loss: -0.012410346418619156 - eval loss: -0.03556123003363609
STEP 5430 training loss: -0.04098208621144295 - eval loss: -0.03556123003363609
STEP 5440 training loss: -0.020144840702414513 - eval loss: -0.03556123003363609
STEP 5450 training loss: -0.049632903188467026 - eval loss: -0.03556123003363609
STEP 5460 training loss: -0.019666343927383423 - eval loss: -0.03556123003363609
STEP 5470 training loss: -0.04069500043988228 - eval loss: -0.03556123003363609
STEP 5480 training loss: -0.03375297039747238 - eval loss: -0.03556123003363609
STEP 5490 training loss: -0.011707352474331856 - eval loss: -0.03556123003363609
STEP 5500 training loss: -0.02037692256271839 - eval loss: -0.03556123003363609
STEP 5510 training loss: -0.03956161066889763 - eval loss: -0.03556123003363609
STEP 5520 training loss: -0.0715438649058342 - eval loss: -0.03556123003363609
STEP 5530 training loss: -0.046670880168676376 - eval loss: -0.03556123003363609
STEP 5540 training loss: -0.040110792964696884 - eval loss: -0.03556123003363609
STEP 5550 training loss: -0.012321712449193 - eval loss: -0.03556123003363609
STEP 5560 training loss: -0.02758474461734295 - eval loss: -0.03556123003363609
STEP 5570 training loss: -0.02617534063756466 - eval loss: -0.03556123003363609
STEP 5580 training loss: -0.04878989979624748 - eval loss: -0.03556123003363609
STEP 5590 training loss: -0.03528258576989174 - eval loss: -0.03556123003363609
STEP 5600 training loss: -0.01304065715521574 - eval loss: -0.03556123003363609
STEP 5610 training loss: -0.08495514839887619 - eval loss: -0.03556123003363609
STEP 5620 training loss: -0.05244912579655647 - eval loss: -0.03556123003363609
STEP 5630 training loss: -0.03415970131754875 - eval loss: -0.03556123003363609
STEP 5640 training loss: -0.029186174273490906 - eval loss: -0.03556123003363609
STEP 5650 training loss: -0.02853877656161785 - eval loss: -0.03556123003363609
STEP 5660 training loss: -0.009005607105791569 - eval loss: -0.03556123003363609
STEP 5670 training loss: -0.07909512519836426 - eval loss: -0.03556123003363609
STEP 5680 training loss: -0.07848110049962997 - eval loss: -0.03556123003363609
STEP 5690 training loss: -0.026089733466506004 - eval loss: -0.03556123003363609
STEP 5700 training loss: -0.03866976499557495 - eval loss: -0.03556123003363609
STEP 5710 training loss: -0.01373804546892643 - eval loss: -0.03556123003363609
STEP 5720 training loss: -0.026773229241371155 - eval loss: -0.03556123003363609
STEP 5730 training loss: -0.018956318497657776 - eval loss: -0.03556123003363609
STEP 5740 training loss: -0.011954891495406628 - eval loss: -0.03556123003363609
STEP 5750 training loss: -0.020580284297466278 - eval loss: -0.03556123003363609
STEP 5760 training loss: -0.04511749744415283 - eval loss: -0.03556123003363609
STEP 5770 training loss: -0.01272681076079607 - eval loss: -0.03556123003363609
STEP 5780 training loss: -0.041529152542352676 - eval loss: -0.03556123003363609
STEP 5790 training loss: -0.03733908385038376 - eval loss: -0.03556123003363609
STEP 5800 training loss: -0.01501199509948492 - eval loss: -0.03556123003363609
STEP 5810 training loss: -0.038971614092588425 - eval loss: -0.03556123003363609
STEP 5820 training loss: -0.04475660249590874 - eval loss: -0.03556123003363609
STEP 5830 training loss: -0.03761623054742813 - eval loss: -0.03556123003363609
STEP 5840 training loss: -0.04018425568938255 - eval loss: -0.03556123003363609
STEP 5850 training loss: -0.1318187266588211 - eval loss: -0.03556123003363609
STEP 5860 training loss: -0.031187981367111206 - eval loss: -0.03556123003363609
STEP 5870 training loss: -0.05346275493502617 - eval loss: -0.03556123003363609
STEP 5880 training loss: -0.04778750240802765 - eval loss: -0.03556123003363609
STEP 5890 training loss: -0.026461029425263405 - eval loss: -0.03556123003363609
STEP 5900 training loss: -0.10488962382078171 - eval loss: -0.03556123003363609
STEP 5910 training loss: -0.010530853644013405 - eval loss: -0.03556123003363609
STEP 5920 training loss: -0.02817995660007 - eval loss: -0.03556123003363609
STEP 5930 training loss: -0.043272972106933594 - eval loss: -0.03556123003363609
STEP 5940 training loss: -0.06745484471321106 - eval loss: -0.03556123003363609
STEP 5950 training loss: -0.058011557906866074 - eval loss: -0.03556123003363609
STEP 5960 training loss: -0.024284474551677704 - eval loss: -0.03556123003363609
STEP 5970 training loss: -0.00728196045383811 - eval loss: -0.03556123003363609
STEP 5980 training loss: -0.004966826643794775 - eval loss: -0.03556123003363609
STEP 5990 training loss: -0.09179586172103882 - eval loss: -0.03556123003363609
STEP 6000 training loss: -0.07599008083343506 - eval loss: -0.03556123003363609
STEP 6010 training loss: -0.02654438652098179 - eval loss: -0.03556123003363609
STEP 6020 training loss: -0.026711324229836464 - eval loss: -0.03556123003363609
STEP 6030 training loss: -0.009413943625986576 - eval loss: -0.03556123003363609
STEP 6040 training loss: -0.014075438492000103 - eval loss: -0.03556123003363609
STEP 6050 training loss: -0.02694213204085827 - eval loss: -0.03556123003363609
STEP 6060 training loss: -0.021120572462677956 - eval loss: -0.03556123003363609
STEP 6070 training loss: -0.06432825326919556 - eval loss: -0.03556123003363609
STEP 6080 training loss: -0.020553013309836388 - eval loss: -0.03556123003363609
STEP 6090 training loss: -0.05396565422415733 - eval loss: -0.03556123003363609
STEP 6100 training loss: -0.025592459365725517 - eval loss: -0.03556123003363609
STEP 6110 training loss: -0.015662865713238716 - eval loss: -0.03556123003363609
STEP 6120 training loss: -0.010240296833217144 - eval loss: -0.03556123003363609
STEP 6130 training loss: -0.02726101316511631 - eval loss: -0.03556123003363609
STEP 6140 training loss: -0.05085287243127823 - eval loss: -0.03556123003363609
STEP 6150 training loss: -0.030724478885531425 - eval loss: -0.03556123003363609
STEP 6160 training loss: -0.04101979732513428 - eval loss: -0.03556123003363609
STEP 6170 training loss: -0.027859119698405266 - eval loss: -0.03556123003363609
STEP 6180 training loss: -0.04383125901222229 - eval loss: -0.03556123003363609
STEP 6190 training loss: -0.018384531140327454 - eval loss: -0.03556123003363609
STEP 6200 training loss: -0.00997877772897482 - eval loss: -0.03556123003363609
STEP 6210 training loss: -0.010339507833123207 - eval loss: -0.03556123003363609
STEP 6220 training loss: -0.020741593092679977 - eval loss: -0.03556123003363609
STEP 6230 training loss: -0.018718576058745384 - eval loss: -0.03556123003363609
STEP 6240 training loss: -0.050598978996276855 - eval loss: -0.03556123003363609
STEP 6250 training loss: -0.03552594035863876 - eval loss: -0.03556123003363609
STEP 6260 training loss: -0.0383981317281723 - eval loss: -0.03556123003363609
STEP 6270 training loss: -0.0381925068795681 - eval loss: -0.03556123003363609
STEP 6280 training loss: -0.050422292202711105 - eval loss: -0.03556123003363609
STEP 6290 training loss: -0.016637342050671577 - eval loss: -0.03556123003363609
STEP 6300 training loss: -0.07658036798238754 - eval loss: -0.03556123003363609
STEP 6310 training loss: -0.02716267667710781 - eval loss: -0.03556123003363609
STEP 6320 training loss: -0.009881656616926193 - eval loss: -0.03556123003363609
STEP 6330 training loss: -0.04737641662359238 - eval loss: -0.03556123003363609
STEP 6340 training loss: -0.02149813435971737 - eval loss: -0.03556123003363609
STEP 6350 training loss: -0.04293917492032051 - eval loss: -0.03556123003363609
STEP 6360 training loss: -0.022565269842743874 - eval loss: -0.03556123003363609
STEP 6370 training loss: -0.031190788373351097 - eval loss: -0.03556123003363609
STEP 6380 training loss: -0.00522141857072711 - eval loss: -0.03556123003363609
STEP 6390 training loss: -0.04142098128795624 - eval loss: -0.03556123003363609
STEP 6400 training loss: -0.017773425206542015 - eval loss: -0.03556123003363609
STEP 6410 training loss: -0.05933116003870964 - eval loss: -0.03556123003363609
STEP 6420 training loss: -0.008756152354180813 - eval loss: -0.03556123003363609
STEP 6430 training loss: -0.022171899676322937 - eval loss: -0.03556123003363609
STEP 6440 training loss: -0.05028737336397171 - eval loss: -0.03556123003363609
STEP 6450 training loss: -0.028555897995829582 - eval loss: -0.03556123003363609
STEP 6460 training loss: -0.040559321641922 - eval loss: -0.03556123003363609
STEP 6470 training loss: -0.035966455936431885 - eval loss: -0.03556123003363609
STEP 6480 training loss: -0.004937485326081514 - eval loss: -0.03556123003363609
STEP 6490 training loss: -0.03502991050481796 - eval loss: -0.03556123003363609
STEP 6500 training loss: -0.038262851536273956 - eval loss: -0.03556123003363609
STEP 6510 training loss: -0.11888815462589264 - eval loss: -0.03556123003363609
STEP 6520 training loss: -0.04164081811904907 - eval loss: -0.03556123003363609
STEP 6530 training loss: -0.016001084819436073 - eval loss: -0.03556123003363609
STEP 6540 training loss: -0.028549710288643837 - eval loss: -0.03556123003363609
STEP 6550 training loss: -0.018774503841996193 - eval loss: -0.03556123003363609
STEP 6560 training loss: -0.02824566140770912 - eval loss: -0.03556123003363609
STEP 6570 training loss: -0.021440604701638222 - eval loss: -0.03556123003363609
STEP 6580 training loss: -0.0225477684289217 - eval loss: -0.03556123003363609
STEP 6590 training loss: -0.02269860729575157 - eval loss: -0.03556123003363609
STEP 6600 training loss: -0.04219401627779007 - eval loss: -0.03556123003363609
STEP 6610 training loss: -0.03340021148324013 - eval loss: -0.03556123003363609
STEP 6620 training loss: -0.0023011399898678064 - eval loss: -0.03556123003363609
STEP 6630 training loss: -0.07112137228250504 - eval loss: -0.03556123003363609
STEP 6640 training loss: -0.010499933734536171 - eval loss: -0.03556123003363609
STEP 6650 training loss: -0.047469060868024826 - eval loss: -0.03556123003363609
STEP 6660 training loss: -0.020238419994711876 - eval loss: -0.03556123003363609
STEP 6670 training loss: -0.014589834026992321 - eval loss: -0.03556123003363609
STEP 6680 training loss: -0.03234051540493965 - eval loss: -0.03556123003363609
STEP 6690 training loss: -0.03347079083323479 - eval loss: -0.03556123003363609
STEP 6700 training loss: -0.02502838335931301 - eval loss: -0.03556123003363609
STEP 6710 training loss: -0.013554900884628296 - eval loss: -0.03556123003363609
STEP 6720 training loss: -0.0034514914732426405 - eval loss: -0.03556123003363609
STEP 6730 training loss: -0.012649289332330227 - eval loss: -0.03556123003363609
STEP 6740 training loss: -0.011645855382084846 - eval loss: -0.03556123003363609
STEP 6750 training loss: -0.05391118675470352 - eval loss: -0.03556123003363609
STEP 6760 training loss: -0.011215298436582088 - eval loss: -0.03556123003363609
STEP 6770 training loss: -0.015259063802659512 - eval loss: -0.03556123003363609
STEP 6780 training loss: -0.008084039203822613 - eval loss: -0.03556123003363609
STEP 6790 training loss: -0.006832900922745466 - eval loss: -0.03556123003363609
STEP 6800 training loss: -0.017410242930054665 - eval loss: -0.03556123003363609
STEP 6810 training loss: -0.012436331249773502 - eval loss: -0.03556123003363609
STEP 6820 training loss: -0.018964475020766258 - eval loss: -0.03556123003363609
STEP 6830 training loss: -0.02859731949865818 - eval loss: -0.03556123003363609
STEP 6840 training loss: -0.018275117501616478 - eval loss: -0.03556123003363609
STEP 6850 training loss: -0.010937809012830257 - eval loss: -0.03556123003363609
STEP 6860 training loss: -0.10633774846792221 - eval loss: -0.03556123003363609
STEP 6870 training loss: -0.015769602730870247 - eval loss: -0.03556123003363609
STEP 6880 training loss: -0.014295393601059914 - eval loss: -0.03556123003363609
STEP 6890 training loss: -0.0024256380274891853 - eval loss: -0.03556123003363609
STEP 6900 training loss: -0.03117368556559086 - eval loss: -0.03556123003363609
STEP 6910 training loss: -0.051670026034116745 - eval loss: -0.03556123003363609
STEP 6920 training loss: -0.05109044536948204 - eval loss: -0.03556123003363609
STEP 6930 training loss: -0.027424825355410576 - eval loss: -0.03556123003363609
STEP 6940 training loss: -0.019192546606063843 - eval loss: -0.03556123003363609
STEP 6950 training loss: -0.019354237243533134 - eval loss: -0.03556123003363609
STEP 6960 training loss: -0.02718868851661682 - eval loss: -0.03556123003363609
STEP 6970 training loss: -0.08683905750513077 - eval loss: -0.03556123003363609
STEP 6980 training loss: -0.06501807272434235 - eval loss: -0.03556123003363609
STEP 6990 training loss: -0.037505023181438446 - eval loss: -0.03556123003363609
STEP 7000 training loss: -0.00902726873755455 - eval loss: -0.03556123003363609
STEP 7010 training loss: -0.03205389529466629 - eval loss: -0.03556123003363609
STEP 7020 training loss: -0.056003388017416 - eval loss: -0.03556123003363609
STEP 7030 training loss: -0.023107049986720085 - eval loss: -0.03556123003363609
STEP 7040 training loss: -0.0057012285105884075 - eval loss: -0.03556123003363609
STEP 7050 training loss: -0.013363203965127468 - eval loss: -0.03556123003363609
STEP 7060 training loss: -0.04257699474692345 - eval loss: -0.03556123003363609
STEP 7070 training loss: -0.05438711866736412 - eval loss: -0.03556123003363609
STEP 7080 training loss: -0.016080671921372414 - eval loss: -0.03556123003363609
STEP 7090 training loss: -0.0387197881937027 - eval loss: -0.03556123003363609
STEP 7100 training loss: -0.009202824905514717 - eval loss: -0.03556123003363609
STEP 7110 training loss: -0.05280717834830284 - eval loss: -0.03556123003363609
STEP 7120 training loss: -0.011051743291318417 - eval loss: -0.03556123003363609
STEP 7130 training loss: -0.06937944144010544 - eval loss: -0.03556123003363609
STEP 7140 training loss: -0.08412189036607742 - eval loss: -0.03556123003363609
STEP 7150 training loss: -0.03319789096713066 - eval loss: -0.03556123003363609
STEP 7160 training loss: -0.04249255731701851 - eval loss: -0.03556123003363609
STEP 7170 training loss: -0.04580269381403923 - eval loss: -0.03556123003363609
STEP 7180 training loss: -0.031501900404691696 - eval loss: -0.03556123003363609
STEP 7190 training loss: -0.007162249181419611 - eval loss: -0.03556123003363609
STEP 7200 training loss: -0.017249351367354393 - eval loss: -0.03556123003363609
STEP 7210 training loss: -0.04148341342806816 - eval loss: -0.03556123003363609
STEP 7220 training loss: -0.03211117908358574 - eval loss: -0.03556123003363609
STEP 7230 training loss: -0.04116073623299599 - eval loss: -0.03556123003363609
STEP 7240 training loss: -0.05778878927230835 - eval loss: -0.03556123003363609
STEP 7250 training loss: -0.03648864105343819 - eval loss: -0.03556123003363609
STEP 7260 training loss: -0.020090339705348015 - eval loss: -0.03556123003363609
STEP 7270 training loss: -0.011570056900382042 - eval loss: -0.03556123003363609
STEP 7280 training loss: -0.045136403292417526 - eval loss: -0.03556123003363609
STEP 7290 training loss: -0.052561577409505844 - eval loss: -0.03556123003363609
STEP 7300 training loss: -0.02780616283416748 - eval loss: -0.03556123003363609
STEP 7310 training loss: -0.015618917532265186 - eval loss: -0.03556123003363609
STEP 7320 training loss: -0.02343105897307396 - eval loss: -0.03556123003363609
STEP 7330 training loss: -0.015478084795176983 - eval loss: -0.03556123003363609
STEP 7340 training loss: -0.04152052849531174 - eval loss: -0.03556123003363609
STEP 7350 training loss: -0.04270908981561661 - eval loss: -0.03556123003363609
STEP 7360 training loss: -0.02985209785401821 - eval loss: -0.03556123003363609
STEP 7370 training loss: -0.060933809727430344 - eval loss: -0.03556123003363609
STEP 7380 training loss: -0.006697006523609161 - eval loss: -0.03556123003363609
STEP 7390 training loss: -0.041851967573165894 - eval loss: -0.03556123003363609
STEP 7400 training loss: -0.013436103239655495 - eval loss: -0.03556123003363609
STEP 7410 training loss: -0.07346806675195694 - eval loss: -0.03556123003363609
STEP 7420 training loss: -0.008599424734711647 - eval loss: -0.03556123003363609
STEP 7430 training loss: -0.013199860230088234 - eval loss: -0.03556123003363609
STEP 7440 training loss: -0.05485686659812927 - eval loss: -0.03556123003363609
STEP 7450 training loss: -0.05644328519701958 - eval loss: -0.03556123003363609
STEP 7460 training loss: -0.011541536077857018 - eval loss: -0.03556123003363609
STEP 7470 training loss: -0.0431043766438961 - eval loss: -0.03556123003363609
STEP 7480 training loss: -0.008435920812189579 - eval loss: -0.03556123003363609
STEP 7490 training loss: -0.01983218640089035 - eval loss: -0.03556123003363609
STEP 7500 training loss: -0.024585213512182236 - eval loss: -0.03556123003363609
STEP 7510 training loss: -0.00941966287791729 - eval loss: -0.03556123003363609
STEP 7520 training loss: -0.01717197336256504 - eval loss: -0.03556123003363609
STEP 7530 training loss: -0.017161931842565536 - eval loss: -0.03556123003363609
STEP 7540 training loss: -0.03819397836923599 - eval loss: -0.03556123003363609
STEP 7550 training loss: -0.029113441705703735 - eval loss: -0.03556123003363609
STEP 7560 training loss: -0.0470564179122448 - eval loss: -0.03556123003363609
STEP 7570 training loss: -0.029944440349936485 - eval loss: -0.03556123003363609
STEP 7580 training loss: -0.0708848163485527 - eval loss: -0.03556123003363609
STEP 7590 training loss: -0.065871462225914 - eval loss: -0.03556123003363609
STEP 7600 training loss: -0.04592312127351761 - eval loss: -0.03556123003363609
STEP 7610 training loss: -0.03522425517439842 - eval loss: -0.03556123003363609
STEP 7620 training loss: -0.027699077501893044 - eval loss: -0.03556123003363609
STEP 7630 training loss: -0.013794404454529285 - eval loss: -0.03556123003363609
STEP 7640 training loss: -0.10230018198490143 - eval loss: -0.03556123003363609
STEP 7650 training loss: -0.02646295167505741 - eval loss: -0.03556123003363609
STEP 7660 training loss: -0.03875104337930679 - eval loss: -0.03556123003363609
STEP 7670 training loss: -0.003968690987676382 - eval loss: -0.03556123003363609
STEP 7680 training loss: -0.023845549672842026 - eval loss: -0.03556123003363609
STEP 7690 training loss: -0.048653893172740936 - eval loss: -0.03556123003363609
STEP 7700 training loss: -0.061991430819034576 - eval loss: -0.03556123003363609
STEP 7710 training loss: -0.025131437927484512 - eval loss: -0.03556123003363609
STEP 7720 training loss: -0.017472298815846443 - eval loss: -0.03556123003363609
STEP 7730 training loss: -0.030850324779748917 - eval loss: -0.03556123003363609
STEP 7740 training loss: -0.019090745598077774 - eval loss: -0.03556123003363609
STEP 7750 training loss: -0.008291481994092464 - eval loss: -0.03556123003363609
STEP 7760 training loss: -0.03727163001894951 - eval loss: -0.03556123003363609
STEP 7770 training loss: -0.023764390498399734 - eval loss: -0.03556123003363609
STEP 7780 training loss: -0.0641942247748375 - eval loss: -0.03556123003363609
STEP 7790 training loss: -0.015065908432006836 - eval loss: -0.03556123003363609
STEP 7800 training loss: -0.01860871911048889 - eval loss: -0.03556123003363609
STEP 7810 training loss: -0.0014111318159848452 - eval loss: -0.03556123003363609
STEP 7820 training loss: -0.017103752121329308 - eval loss: -0.03556123003363609
STEP 7830 training loss: -0.07178770750761032 - eval loss: -0.03556123003363609
STEP 7840 training loss: -0.00518981134518981 - eval loss: -0.03556123003363609
STEP 7850 training loss: -0.04762382432818413 - eval loss: -0.03556123003363609
STEP 7860 training loss: -0.06347858160734177 - eval loss: -0.03556123003363609
STEP 7870 training loss: -0.03843173384666443 - eval loss: -0.03556123003363609
STEP 7880 training loss: -0.018707944080233574 - eval loss: -0.03556123003363609
STEP 7890 training loss: -0.03883226960897446 - eval loss: -0.03556123003363609
STEP 7900 training loss: -0.010127817280590534 - eval loss: -0.03556123003363609
STEP 7910 training loss: -0.02590351738035679 - eval loss: -0.03556123003363609
STEP 7920 training loss: -0.024638056755065918 - eval loss: -0.03556123003363609
STEP 7930 training loss: -0.06450523436069489 - eval loss: -0.03556123003363609
STEP 7940 training loss: -0.06063712388277054 - eval loss: -0.03556123003363609
STEP 7950 training loss: -0.028512010350823402 - eval loss: -0.03556123003363609
STEP 7960 training loss: -0.010454082861542702 - eval loss: -0.03556123003363609
STEP 7970 training loss: -0.010021543130278587 - eval loss: -0.03556123003363609
STEP 7980 training loss: -0.031873561441898346 - eval loss: -0.03556123003363609
STEP 7990 training loss: -0.048939939588308334 - eval loss: -0.03556123003363609
STEP 8000 training loss: -0.03747536242008209 - eval loss: -0.03556123003363609
STEP 8010 training loss: -0.011259193532168865 - eval loss: -0.03556123003363609
STEP 8020 training loss: -0.00448845699429512 - eval loss: -0.03556123003363609
STEP 8030 training loss: -0.019559433683753014 - eval loss: -0.03556123003363609
STEP 8040 training loss: -0.027001122012734413 - eval loss: -0.03556123003363609
STEP 8050 training loss: -0.025777289643883705 - eval loss: -0.03556123003363609
STEP 8060 training loss: -0.02266836166381836 - eval loss: -0.03556123003363609
STEP 8070 training loss: -0.018641620874404907 - eval loss: -0.03556123003363609
STEP 8080 training loss: -0.03083379566669464 - eval loss: -0.03556123003363609
STEP 8090 training loss: -0.03482319042086601 - eval loss: -0.03556123003363609
STEP 8100 training loss: -0.031944602727890015 - eval loss: -0.03556123003363609
STEP 8110 training loss: -0.005804380867630243 - eval loss: -0.03556123003363609
STEP 8120 training loss: -0.01692347228527069 - eval loss: -0.03556123003363609
STEP 8130 training loss: -0.04205901175737381 - eval loss: -0.03556123003363609
STEP 8140 training loss: -0.020857302471995354 - eval loss: -0.03556123003363609
STEP 8150 training loss: -0.012109048664569855 - eval loss: -0.03556123003363609
STEP 8160 training loss: -0.018047261983156204 - eval loss: -0.03556123003363609
STEP 8170 training loss: -0.06193748861551285 - eval loss: -0.03556123003363609
STEP 8180 training loss: -0.009066714905202389 - eval loss: -0.03556123003363609
STEP 8190 training loss: -0.021710297092795372 - eval loss: -0.03556123003363609
STEP 8200 training loss: -0.04949444159865379 - eval loss: -0.03556123003363609
STEP 8210 training loss: -0.09827928990125656 - eval loss: -0.03556123003363609
STEP 8220 training loss: -0.016385139897465706 - eval loss: -0.03556123003363609
STEP 8230 training loss: -0.03678667172789574 - eval loss: -0.03556123003363609
STEP 8240 training loss: -0.07200049608945847 - eval loss: -0.03556123003363609
STEP 8250 training loss: -0.007801604922860861 - eval loss: -0.03556123003363609
STEP 8260 training loss: -0.0348370224237442 - eval loss: -0.03556123003363609
STEP 8270 training loss: -0.03125026449561119 - eval loss: -0.03556123003363609
STEP 8280 training loss: -0.056516628712415695 - eval loss: -0.03556123003363609
STEP 8290 training loss: -0.0691089779138565 - eval loss: -0.03556123003363609
STEP 8300 training loss: -0.00993086863309145 - eval loss: -0.03556123003363609
STEP 8310 training loss: -0.037642594426870346 - eval loss: -0.03556123003363609
STEP 8320 training loss: -0.025895794853568077 - eval loss: -0.03556123003363609
STEP 8330 training loss: -0.04930950328707695 - eval loss: -0.03556123003363609
STEP 8340 training loss: -0.029710471630096436 - eval loss: -0.03556123003363609
STEP 8350 training loss: -0.008864841423928738 - eval loss: -0.03556123003363609
STEP 8360 training loss: -0.06289199739694595 - eval loss: -0.03556123003363609
STEP 8370 training loss: -0.04534947872161865 - eval loss: -0.03556123003363609
STEP 8380 training loss: -0.036996033042669296 - eval loss: -0.03556123003363609
STEP 8390 training loss: -0.026940811425447464 - eval loss: -0.03556123003363609
STEP 8400 training loss: -0.09212642908096313 - eval loss: -0.03556123003363609
STEP 8410 training loss: -0.05265284702181816 - eval loss: -0.03556123003363609
STEP 8420 training loss: -0.01748627983033657 - eval loss: -0.03556123003363609
STEP 8430 training loss: -0.053804706782102585 - eval loss: -0.03556123003363609
STEP 8440 training loss: -0.08329970389604568 - eval loss: -0.03556123003363609
STEP 8450 training loss: -0.0702832043170929 - eval loss: -0.03556123003363609
STEP 8460 training loss: -0.021161435171961784 - eval loss: -0.03556123003363609
STEP 8470 training loss: -0.018053429201245308 - eval loss: -0.03556123003363609
STEP 8480 training loss: -0.02628944255411625 - eval loss: -0.03556123003363609
STEP 8490 training loss: -0.032564591616392136 - eval loss: -0.03556123003363609
STEP 8500 training loss: -0.01368819922208786 - eval loss: -0.03556123003363609
STEP 8510 training loss: -0.015015298500657082 - eval loss: -0.03556123003363609
STEP 8520 training loss: -0.08021442592144012 - eval loss: -0.03556123003363609
STEP 8530 training loss: -0.03444383293390274 - eval loss: -0.03556123003363609
STEP 8540 training loss: -0.02031889371573925 - eval loss: -0.03556123003363609
STEP 8550 training loss: -0.017446449026465416 - eval loss: -0.03556123003363609
STEP 8560 training loss: -0.11279648542404175 - eval loss: -0.03556123003363609
STEP 8570 training loss: -0.04369400814175606 - eval loss: -0.03556123003363609
STEP 8580 training loss: -0.016880398616194725 - eval loss: -0.03556123003363609
STEP 8590 training loss: -0.01363418996334076 - eval loss: -0.03556123003363609
STEP 8600 training loss: -0.031812120229005814 - eval loss: -0.03556123003363609
STEP 8610 training loss: -0.045042503625154495 - eval loss: -0.03556123003363609
STEP 8620 training loss: -0.019190048798918724 - eval loss: -0.03556123003363609
STEP 8630 training loss: -0.05905061960220337 - eval loss: -0.03556123003363609
STEP 8640 training loss: -0.06326170265674591 - eval loss: -0.03556123003363609
STEP 8650 training loss: -0.008867044933140278 - eval loss: -0.03556123003363609
STEP 8660 training loss: -0.024787388741970062 - eval loss: -0.03556123003363609
STEP 8670 training loss: -0.01113162748515606 - eval loss: -0.03556123003363609
STEP 8680 training loss: -0.017012976109981537 - eval loss: -0.03556123003363609
STEP 8690 training loss: -0.017589373514056206 - eval loss: -0.03556123003363609
STEP 8700 training loss: -0.021286647766828537 - eval loss: -0.03556123003363609
STEP 8710 training loss: -0.048458293080329895 - eval loss: -0.03556123003363609
STEP 8720 training loss: -0.009036422707140446 - eval loss: -0.03556123003363609
STEP 8730 training loss: -0.022334333509206772 - eval loss: -0.03556123003363609
STEP 8740 training loss: -0.03193509578704834 - eval loss: -0.03556123003363609
STEP 8750 training loss: -0.01973087526857853 - eval loss: -0.03556123003363609
STEP 8760 training loss: -0.07792799174785614 - eval loss: -0.03556123003363609
STEP 8770 training loss: -0.0015765138668939471 - eval loss: -0.03556123003363609
STEP 8780 training loss: -0.07855307310819626 - eval loss: -0.03556123003363609
STEP 8790 training loss: -0.021498730406165123 - eval loss: -0.03556123003363609
STEP 8800 training loss: -0.007697154767811298 - eval loss: -0.03556123003363609
STEP 8810 training loss: -0.018078316003084183 - eval loss: -0.03556123003363609
STEP 8820 training loss: -0.020061539486050606 - eval loss: -0.03556123003363609
STEP 8830 training loss: -0.024766236543655396 - eval loss: -0.03556123003363609
STEP 8840 training loss: -0.02533472515642643 - eval loss: -0.03556123003363609
STEP 8850 training loss: -0.017148442566394806 - eval loss: -0.03556123003363609
STEP 8860 training loss: -0.057988982647657394 - eval loss: -0.03556123003363609
STEP 8870 training loss: -0.0562596321105957 - eval loss: -0.03556123003363609
STEP 8880 training loss: -0.016759952530264854 - eval loss: -0.03556123003363609
STEP 8890 training loss: -0.03159567341208458 - eval loss: -0.03556123003363609
STEP 8900 training loss: -0.02866954170167446 - eval loss: -0.03556123003363609
STEP 8910 training loss: -0.06651611626148224 - eval loss: -0.03556123003363609
STEP 8920 training loss: -0.009482530876994133 - eval loss: -0.03556123003363609
STEP 8930 training loss: -0.010746367275714874 - eval loss: -0.03556123003363609
STEP 8940 training loss: -0.04329812154173851 - eval loss: -0.03556123003363609
STEP 8950 training loss: -0.006881847511976957 - eval loss: -0.03556123003363609
STEP 8960 training loss: -0.048323243856430054 - eval loss: -0.03556123003363609
STEP 8970 training loss: -0.04721791297197342 - eval loss: -0.03556123003363609
STEP 8980 training loss: -0.07118845731019974 - eval loss: -0.03556123003363609
STEP 8990 training loss: -0.01588924415409565 - eval loss: -0.03556123003363609
STEP 9000 training loss: -0.03291631117463112 - eval loss: -0.03556123003363609
STEP 9010 training loss: -0.047398801892995834 - eval loss: -0.03556123003363609
STEP 9020 training loss: -0.016780328005552292 - eval loss: -0.03556123003363609
STEP 9030 training loss: -0.057094890624284744 - eval loss: -0.03556123003363609
STEP 9040 training loss: -0.013281658291816711 - eval loss: -0.03556123003363609
STEP 9050 training loss: -0.08291368931531906 - eval loss: -0.03556123003363609
STEP 9060 training loss: -0.04981149733066559 - eval loss: -0.03556123003363609
STEP 9070 training loss: -0.05527700111269951 - eval loss: -0.03556123003363609
STEP 9080 training loss: -0.007644061930477619 - eval loss: -0.03556123003363609
STEP 9090 training loss: -0.008209450170397758 - eval loss: -0.03556123003363609
STEP 9100 training loss: -0.04296554997563362 - eval loss: -0.03556123003363609
STEP 9110 training loss: -0.005428082775324583 - eval loss: -0.03556123003363609
STEP 9120 training loss: -0.0817614272236824 - eval loss: -0.03556123003363609
STEP 9130 training loss: -0.013778293505311012 - eval loss: -0.03556123003363609
STEP 9140 training loss: -0.03234259411692619 - eval loss: -0.03556123003363609
STEP 9150 training loss: -0.025830714032053947 - eval loss: -0.03556123003363609
STEP 9160 training loss: -0.017170194536447525 - eval loss: -0.03556123003363609
STEP 9170 training loss: -0.056531842797994614 - eval loss: -0.03556123003363609
STEP 9180 training loss: -0.04279246926307678 - eval loss: -0.03556123003363609
STEP 9190 training loss: -0.03558981418609619 - eval loss: -0.03556123003363609
STEP 9200 training loss: -0.04436813294887543 - eval loss: -0.03556123003363609
STEP 9210 training loss: -0.004647145513445139 - eval loss: -0.03556123003363609
STEP 9220 training loss: -0.06779233366250992 - eval loss: -0.03556123003363609
STEP 9230 training loss: -0.023329241201281548 - eval loss: -0.03556123003363609
STEP 9240 training loss: -0.04561146721243858 - eval loss: -0.03556123003363609
STEP 9250 training loss: -0.013901648111641407 - eval loss: -0.03556123003363609
STEP 9260 training loss: -0.03650882467627525 - eval loss: -0.03556123003363609
STEP 9270 training loss: -0.03670549392700195 - eval loss: -0.03556123003363609
STEP 9280 training loss: -0.04026949033141136 - eval loss: -0.03556123003363609
STEP 9290 training loss: -0.03443583473563194 - eval loss: -0.03556123003363609
STEP 9300 training loss: -0.015290471725165844 - eval loss: -0.03556123003363609
STEP 9310 training loss: -0.023633385077118874 - eval loss: -0.03556123003363609
STEP 9320 training loss: -0.043472956866025925 - eval loss: -0.03556123003363609
STEP 9330 training loss: -0.034336917102336884 - eval loss: -0.03556123003363609
STEP 9340 training loss: -0.011610573157668114 - eval loss: -0.03556123003363609
STEP 9350 training loss: -0.008536132983863354 - eval loss: -0.03556123003363609
STEP 9360 training loss: -0.05792153626680374 - eval loss: -0.03556123003363609
STEP 9370 training loss: -0.02455512247979641 - eval loss: -0.03556123003363609
STEP 9380 training loss: -0.029295343905687332 - eval loss: -0.03556123003363609
STEP 9390 training loss: -0.02461780048906803 - eval loss: -0.03556123003363609
STEP 9400 training loss: -0.022840170189738274 - eval loss: -0.03556123003363609
STEP 9410 training loss: -0.014772321097552776 - eval loss: -0.03556123003363609
STEP 9420 training loss: -0.017181407660245895 - eval loss: -0.03556123003363609
STEP 9430 training loss: -0.02345133386552334 - eval loss: -0.03556123003363609
STEP 9440 training loss: -0.02573295496404171 - eval loss: -0.03556123003363609
STEP 9450 training loss: -0.09614818543195724 - eval loss: -0.03556123003363609
STEP 9460 training loss: -0.04524042084813118 - eval loss: -0.03556123003363609
STEP 9470 training loss: -0.023626454174518585 - eval loss: -0.03556123003363609
STEP 9480 training loss: -0.02058647945523262 - eval loss: -0.03556123003363609
STEP 9490 training loss: -0.025251949205994606 - eval loss: -0.03556123003363609
STEP 9500 training loss: -0.06288788467645645 - eval loss: -0.03556123003363609
STEP 9510 training loss: -0.010154406540095806 - eval loss: -0.03556123003363609
STEP 9520 training loss: -0.004091703332960606 - eval loss: -0.03556123003363609
STEP 9530 training loss: -0.011274672113358974 - eval loss: -0.03556123003363609
STEP 9540 training loss: -0.010674651712179184 - eval loss: -0.03556123003363609
STEP 9550 training loss: -0.04573885351419449 - eval loss: -0.03556123003363609
STEP 9560 training loss: -0.06566894054412842 - eval loss: -0.03556123003363609
STEP 9570 training loss: -0.016028188169002533 - eval loss: -0.03556123003363609
STEP 9580 training loss: -0.023418201133608818 - eval loss: -0.03556123003363609
STEP 9590 training loss: -0.039569053798913956 - eval loss: -0.03556123003363609
STEP 9600 training loss: -0.023412762209773064 - eval loss: -0.03556123003363609
STEP 9610 training loss: -0.02193143032491207 - eval loss: -0.03556123003363609
STEP 9620 training loss: -0.004996002651751041 - eval loss: -0.03556123003363609
STEP 9630 training loss: -0.027167636901140213 - eval loss: -0.03556123003363609
STEP 9640 training loss: -0.029923344030976295 - eval loss: -0.03556123003363609
STEP 9650 training loss: -0.04481307789683342 - eval loss: -0.03556123003363609
STEP 9660 training loss: -0.038512732833623886 - eval loss: -0.03556123003363609
STEP 9670 training loss: -0.049409251660108566 - eval loss: -0.03556123003363609
STEP 9680 training loss: -0.04803798720240593 - eval loss: -0.03556123003363609
STEP 9690 training loss: -0.011163108982145786 - eval loss: -0.03556123003363609
STEP 9700 training loss: -0.002702021272853017 - eval loss: -0.03556123003363609
STEP 9710 training loss: -0.04287652671337128 - eval loss: -0.03556123003363609
STEP 9720 training loss: -0.020813215523958206 - eval loss: -0.03556123003363609
STEP 9730 training loss: -0.07819443196058273 - eval loss: -0.03556123003363609
STEP 9740 training loss: -0.0229248758405447 - eval loss: -0.03556123003363609
STEP 9750 training loss: -0.028726650401949883 - eval loss: -0.03556123003363609
STEP 9760 training loss: -0.019979415461421013 - eval loss: -0.03556123003363609
STEP 9770 training loss: -0.039032552391290665 - eval loss: -0.03556123003363609
STEP 9780 training loss: -0.01029034610837698 - eval loss: -0.03556123003363609
STEP 9790 training loss: -0.03684035316109657 - eval loss: -0.03556123003363609
STEP 9800 training loss: -0.038333456963300705 - eval loss: -0.03556123003363609
STEP 9810 training loss: -0.06672712415456772 - eval loss: -0.03556123003363609
STEP 9820 training loss: -0.03106197528541088 - eval loss: -0.03556123003363609
STEP 9830 training loss: -0.031252577900886536 - eval loss: -0.03556123003363609
STEP 9840 training loss: -0.046418193727731705 - eval loss: -0.03556123003363609
STEP 9850 training loss: -0.027542516589164734 - eval loss: -0.03556123003363609
STEP 9860 training loss: -0.002581440145149827 - eval loss: -0.03556123003363609
STEP 9870 training loss: -0.05289887264370918 - eval loss: -0.03556123003363609
STEP 9880 training loss: -0.024919748306274414 - eval loss: -0.03556123003363609
STEP 9890 training loss: -0.06971395015716553 - eval loss: -0.03556123003363609
STEP 9900 training loss: -0.05116063356399536 - eval loss: -0.03556123003363609
STEP 9910 training loss: -0.005867773201316595 - eval loss: -0.03556123003363609
STEP 9920 training loss: -0.019203996285796165 - eval loss: -0.03556123003363609
STEP 9930 training loss: -0.010442020371556282 - eval loss: -0.03556123003363609
STEP 9940 training loss: -0.005127841141074896 - eval loss: -0.03556123003363609
STEP 9950 training loss: -0.07332159578800201 - eval loss: -0.03556123003363609
STEP 9960 training loss: -0.03663865849375725 - eval loss: -0.03556123003363609
STEP 9970 training loss: -0.04629722237586975 - eval loss: -0.03556123003363609
STEP 9980 training loss: -0.08286865055561066 - eval loss: -0.03556123003363609
STEP 9990 training loss: -0.06381454318761826 - eval loss: -0.03556123003363609
STEP 10000 training loss: -0.039056289941072464 - eval loss: -0.03556123003363609